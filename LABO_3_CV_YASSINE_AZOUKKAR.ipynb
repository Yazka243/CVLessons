{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP40Am9VC8G0ApAfxlcjXB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yazka243/CVLessons/blob/labo3/LABO_3_CV_YASSINE_AZOUKKAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkQchSpd5pdx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 The pinhole camera model**\n",
        "\n",
        "Essential to understanding image formation is the pinhole camera model, that describes the projection of the world onto the image sensor in the simplest form. It is a projection model from 3D world geometry onto a 2D plane. Rays of light originating (i.e., reflected by) the world geometry pass through an infinitesimally small hole and then intersect with a plane. The coordinates of this point of intersection determine the image coordinates. The process is illustrated in Figure 1.\n",
        "\n",
        "![Pinhole camera image formation](https://github.com/Yazka243/CVLessons/blob/labo3/LABO_3_FIG_1.png?raw=true)\n",
        "\n",
        ">The hole is called the focal point of the projection, and the line through the focal point, perpendicular to the image plane is called the principal axis. The mathematical description of the projection is very straightforward using similar triangles, as shown in Figure 2.\n",
        "\n",
        "![Similar triangle in pinhole model](https://github.com/Yazka243/CVLessons/blob/labo3/LABO_3_FIG_2.png?raw=true)\n",
        "\n",
        ">As can be seen from the figure, the following relationships hold:\n",
        "\n",
        "![for1](https://github.com/Yazka243/CVLessons/blob/labo3/LABO_3_FIG_3.png?raw=true)\n",
        "\n",
        ">or written differently:\n",
        "\n",
        "![for2](https://github.com/Yazka243/CVLessons/blob/labo3/LABO_3_FIG_4.png?raw=true)\n",
        "\n",
        ">In an actual camera, the image plane is made up from photosensitive elements that accumulate the incoming light within a rectangular (usually square) region; they are essentially photon counters. These elements are called pixels (short for picture elements), and the images are described in discrete pixel coordinates rather than the continuous x and y expressed in meters in the equations and figure above. Note that because of the similar triangles, the scale and units of the focal distance and image coordinates are not important, only their ratio is. We can therefore express the focal distance f in pixels, e.g., if the focal distance is 5mm and the physical size of one photosensitive square on the sensor s 5μm, we can use 1000 as focal distance and the quantities x and y will then be the horizontal and vertical deviation, in pixels, from the principal axis. They now describe pixel coordinates relative to the principal point (the place where the principal axis intersects the image plane). They differ from the image coordinates you use in python to address the images only in a translation: since the origin (0, 0) of the image in python is at the top left corner, we have to add half the horizontal and vertical size of the image (in pixels) to x and y.\n",
        "\n",
        ">The scaling and translation operation, as well as the division by Z are elegantly described in homogeneous coordinates by the following matrix operation:\n",
        "\n",
        "![for3](https://github.com/Yazka243/CVLessons/blob/labo3/LABO_3_FIG_5.png?raw=true)\n",
        "\n",
        "\n",
        "Note that the actual projection is in the conversion from homogeneous coordinates back to regular coordinates. After multiplying the matrix with the 3D coordinates, the third value is simply the Z value, i.e., the distance to the object. To convert to regular coordinates, you divide by the third value, meaning an object that was twice as far, appears half as large.\n",
        "\n",
        ">The 3x3 matrix in the middle is called the intrinsic camera matrix. For a sensor with rectangular pixels or a less than perfect lens, the horizontal and vertical focal distances may be different. cx and cy representthe coordinates of the principal point, in pixels. This point is usually near the center of the sensor, but tolerances in camera assembly can shift it; no two cameras are exactly the same."
      ],
      "metadata": {
        "id": "SEf0er727AU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 1**\n",
        "Write a simple camera simulator that projects a 3D wireframe model to image coordinates."
      ],
      "metadata": {
        "id": "Ute6jvex8kS7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BafALarZ_cDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 1** What is the camera matrix for a 1080p camera with a horizontal field of view of 90 degrees?\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "dhZP3pnD_cYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Assignment 1**\n",
        "* Create a virtual 3D cube with a side of 1 meter, defined as an 8x3 matrix containing the 3D coordinates\n",
        "of the vertices of the cube relative to the camera (you can choose the exact position, picture the situation\n",
        "in your head or make a quick freehand diagram), and a 12x2 array of edges described by pairs of vertex\n",
        "indices that need to be connected by lines;\n",
        "* Project the 3D vertex coordinates to 2D image coordinates using your camera matrix from Question\n",
        "1, and visualize the result by drawing the vertices and edges on an empty 1080p image. Pay attention\n",
        "to the dimensions of your matrices, transpose as necessary and round the image coordinates to integer\n",
        "pixels.\n",
        "\n",
        "Figure 3 shows an example of a cube projected like this.\n",
        "\n",
        "Tip: use numpy for the mathematics and the opencv drawing functions line and circle.\n",
        "\n",
        "![figure3](https://github.com/Yazka243/CVLessons/blob/labo3/LABO_3_FIG_6.png?raw=true)"
      ],
      "metadata": {
        "id": "2_BKev6H_jed"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2UbuowSAAEU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 2** If you double the focal distance, what happens to the picture?\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "oxvB7A3nAEzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 Camera calibration**\n",
        "\n",
        "More advanced tasks in computer vision, such as stereo vision, pose estimation and structure from motion build on the pinhole camera model and therefore require us to determine the camera matrix for a given camera and lens combination. Important to note is that the pinhole model is a poor approximation in practice: lenses do not behave like an infinitesimally small hole. A lens achieves the same purpose (focusing light coming from one direction onto one place on the image sensor) by refracting incoming light beams, which allows for a lot more light to come through (the aperture can be much larger) but also induces distortion: the simple relation defined by the similar triangles no longer holds.\n",
        "\n",
        ">There are various types of distortion, but the dominant effect is radial distortion, where the light rays are\n",
        "deflected radially: incident rays that form a large angle with the principal axis, seem to have a different focal\n",
        "distance than rays that are closer to the principal axis. Rather than have a focal distance that depends on the\n",
        "incidence angle, this type of distortion is compensated for by a transformation that converts the real image\n",
        "coordinates into pinhole model coordinates. The transformation is given by a polynomial correction. Let r be\n",
        "the distance of the original image point (x, y) to the principal point, i.e. r =\n",
        "p\n",
        "(xorig − cx)2 + (yorig − cy)2.\n",
        "The corrected coordinates of the point are then:\n",
        "\n",
        "![for4](https://github.com/Yazka243/CVLessons/blob/labo3/LABO_3_FIG_7.png?raw=true)\n",
        "\n",
        "This transformation moves the point along the radial line through the principal point along a distance specified by the distortion coefficients k1 and k2. More than two coefficients can be used (for higher powers of r) but in practice this is not usually required."
      ],
      "metadata": {
        "id": "yndLFqV5AMnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 3** Why are there only even powers in this polynomial in r?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The most common method to simultaneously determine the camera matrix and the distortion coefficients is Zhang’s method, which is an iterative procedure that estimates the intrinsic matrix from many different views of a known object assuming no distortion, then estimates the distortion and then refines all parameters using a non-linear optimization technique."
      ],
      "metadata": {
        "id": "qzrx9_lXA3VT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 2**\n",
        "Calibrate a camera based on checkerboard pattern views."
      ],
      "metadata": {
        "id": "ZSbGGMsKBRuK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4JByLr9eBaic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Assignment 2** Determine the intrinsic matrix and distortion parameters of the gopro camera used to shoot the calibration sequence you find on Ufora as **calibration_frames.zip**. Print the matrix and coefficients in your report."
      ],
      "metadata": {
        "id": "7IkEqZK_Ba3a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEjO0bwOBh7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Assignment 3** Do the calibration procedure for different random subsets of 20 frames. Print the standard deviation across these calibration experiments on each of the parameters."
      ],
      "metadata": {
        "id": "ZICIzEY1BiM6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3EXxPaWBqyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Assignment 4** Use your best calibration result to undistort one of the frames from the sequence. Lines that are straight in reality should be straight in your rectified image now.\n",
        "\n",
        "Tip: there is a camera calibration example in the opencv documentation."
      ],
      "metadata": {
        "id": "SLezUezHBrs-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "so0Vr2YWB01N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 Geometric transformations**\n",
        "In this lab we discuss some types of two-dimensional geometric transformations. These are transformations that convert a plane into another plane. Examples are translation, rotation, scaling, affine transformation and perspective transformation.\n",
        "\n",
        "\n",
        ">Translation, rotation and scaling speak for themselves. Affine transformations are all transformations that maintain collinearity and distance relationships. Each affine transformation can be seen as a combination of translation, rotation, scaling, and shear. Perspective transformations (also called homographies) are transformations that only retain linearity.\n",
        "\n",
        "![Figure4](https://github.com/Yazka243/CVLessons/blob/labo3/LABO_3_FIG_8.png?raw=true)"
      ],
      "metadata": {
        "id": "3T57ZPMgB1lc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3**\n",
        "Write a program that applies a horizontal shear transform to an image. The transformation matrix to shear horizontally has this form:\n",
        "\n",
        "![for5](https://github.com/Yazka243/CVLessons/blob/labo3/LABO_3_FIG_9.png?raw=true)\n",
        "\n",
        "where m is the shear factor. Note the dimensions: this is an image-to-image transform, but it has 3 columns, meaning it applies to *homogeneous coordinates*."
      ],
      "metadata": {
        "id": "bFQTwD3MCPtr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQwqb2GTCgU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Assignment 5** Shear *shadow.png* so that the photographer’s shadow becomes vertical. Size your target image so that it will be large enough to accommodate the sheared image, and make sure that all parts of the original image are visible. You can add translation by placing pixel offsets in the third column.\n",
        "\n",
        "\n",
        "New functions: **warpAffine**."
      ],
      "metadata": {
        "id": "bqYBENuNCg0t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KUCnIFRkCwVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 4**\n",
        "Write a program that applies a perspective transform to obtain a perpendicular view on the ground plane, also called a bird’s eye view. Unlike a rotation, scaling or shear transform, a perspective transform is not trivial to define manually. Usually, you will determine a perspective transform by solving for it: you specify a set of source coordinates and target coordinates for a set of points, and the transformation that maps one onto the other is determined by solving the following system:\n",
        "\n",
        "![for6](https://github.com/Yazka243/CVLessons/blob/labo3/LABO_3_FIG_10.png?raw=true)\n",
        "\n",
        "This system has 8 degrees of freedom (it is only defined up to a scale factor, which does not matter in homogeneous coordinates), so you need source and target coordinates for at least 4 points (resulting in 4 equations in x and 4 equations in y).\n"
      ],
      "metadata": {
        "id": "J4FwYfQQC0G-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FvZtcKoRENgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Assignment 6** Apply a perspective transform to **shadow_box.png** so that the photographer in not only stands vertically, but is also proportionally correct. In your program you click on the 4 corners of the tetragon that you want to transform into a rectangle, after which the right perspective transformation is searched for and executed. In order to make an image display window clickable, you have to set a mouse callback function that is called anytime your mouse pointer interacts in some way with the window.\n",
        "\n",
        "Tip: an example of a mouse callback function can be found in callback.py on Ufora. Functions you need: **setMouseCallback**, **getPerspectiveTransform**, **warpPerspective**.\n",
        "\n",
        "![figure5](https://github.com/Yazka243/CVLessons/blob/labo3/LABO_3_FIG_11.png?raw=true)"
      ],
      "metadata": {
        "id": "TgKINLGiENzD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ACuB7KPYEPR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4** If you were to specify more than 4 points, the system would be overdetermined. How can you solve such an overdetermined system?\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "cjCAPMrTEPoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 5** Can you find an opencv function that takes more than 4 points?\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "dKtU_SlgEVVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4 Report**\n",
        "Write a brief report about this lab session in which you answer each question and provide the output image of each assignment. Append your source code."
      ],
      "metadata": {
        "id": "2MoZYSYiEd-G"
      }
    }
  ]
}